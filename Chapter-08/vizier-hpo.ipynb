{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70b676c-f315-4489-b5b3-c593d32aa187",
   "metadata": {},
   "source": [
    "## Environment setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c0fee-3e4f-46da-bd7d-a7e82098c8e4",
   "metadata": {},
   "source": [
    "Let's ensure we're using the latest version of the Vertex AI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd39e7-ef4b-4c9e-8577-b194c862034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade xgboost google-cloud-aiplatform --user -q --no-warn-script-location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257ddb9-b978-4209-9930-7e16c4294759",
   "metadata": {},
   "source": [
    "## Restart the kernel after install\n",
    "\n",
    "*After restart, continue to the cells below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae1917-563e-406f-8e30-6d9e9b55ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2243db-4e48-40e9-847e-6f4b9534144a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "Using Vertex AI Vizier for hyperparameter tuning involves several steps. First, we'll need to create a training application, which will consists of a Python script that trains our model with given hyperparameters and then saves the trained model. This script must also report the performance of the model on the validation set, so Vertex AI Vizier can determine the best hyperparameters.\n",
    "\n",
    "Next, we need to create a configuration file for the hyperparameter tuning job, which specifies the hyperparameters to tune and their possible values, as well as the metric to optimize.\n",
    "\n",
    "Finally, we'll use the Vertex AI Vizier client library to submit the hyperparameter tuning job, which will run our training application with different sets of hyperparameters, and find the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0e2eb-d8a5-42fe-820e-9a158e01b811",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08736d-fc08-4733-ac24-fa654466f7bc",
   "metadata": {},
   "source": [
    "Let's set some inmitial variables and put our dataset in place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a8002-7b02-4356-803b-611710b10082",
   "metadata": {},
   "source": [
    "Set variables related to our environment.\n",
    "\n",
    "* Replace **YOUR_PROJECT_ID** with your project ID.\n",
    "* Replace **YOUR_PREFERRED_REGION** with your preferred region.\n",
    "* Replace **YOUR_BUCKET_NAME** with your bucket name. (Hint: you can use any bucket you created already in this book.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd487e9-cfb9-4cc1-8e87-0c0d6608626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID=\"YOUR_PROJECT_ID\"\n",
    "REGION=\"YOUR_REGION\"\n",
    "BUCKET=\"YOUR_BUCKET_NAME\"\n",
    "BUCKET_URI=f\"gs://{BUCKET}\"\n",
    "APP_NAME=\"fraud-detect\"\n",
    "APPLICATION_DIR = \"vizier\"\n",
    "TRAINER_DIR = f\"{APPLICATION_DIR}/trainer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31365ee2-fc8d-46a4-bb98-e30203c3f985",
   "metadata": {},
   "source": [
    "Copy the dataset to GCS so our code can access it later:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ede06-29f5-4685-800b-bc7bb5b52a68",
   "metadata": {},
   "source": [
    "## Get the source data for this use case\n",
    "\n",
    "1. Download the \"Credit Card Fraud Detection\" dataset from [this link](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/download?datasetVersionNumber=3) (clicking on the link should automatically download it to your local computer). \n",
    "2. The file downloads as a zip archive, so you will need to extract the creditcard.csv from within that zip archive.\n",
    "3. In the top-left corner your JupyterLab screen – i.e., the screen on which you are currently reading these instructions – click upload symbol (the symbol is an arrow pointing upwards).\n",
    "4. Upload the creditcard.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b507d87-24b8-40b0-bb09-415f5b14f24e",
   "metadata": {},
   "source": [
    "## Transfer the data to GCS\n",
    "\n",
    "Run the following command to transfer the data to GCS to be referenced in our training script later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66c23e-f81b-4251-b011-74437682799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp creditcard.csv $BUCKET_URI/creditcard.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76022d02-610a-40ec-9225-d7a3690a4d29",
   "metadata": {},
   "source": [
    "## Containerize the training application code\n",
    "\n",
    "Before we can run a hyperparameter tuning job, we need to create a source code file (training script) and a Dockerfile. The source code trains a model using XGBoost, and the Dockerfile will include all the commands needed to run the container image.\n",
    "\n",
    "It will install all of the libraries required by our training script, and set up the entry point for the training code.\n",
    "\n",
    "First, let's create a couple of directories that we'll use, and import and initialize the Google Cloud AI Platfrom client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2206f-d695-4507-b648-0ab2c5059d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $APPLICATION_DIR\n",
    "!mkdir -p $TRAINER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b303785-8351-497f-8041-70011174d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b7a18-1555-43c1-9499-219ac7ab9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AI Platform client\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58e964-2b26-4767-95fd-3fdd6cd5678a",
   "metadata": {},
   "source": [
    "### Create the training application (train.py)\n",
    "\n",
    "The code in the next cell will create our training script.\n",
    "\n",
    "**Important notes for our training code:**\n",
    "\n",
    "*Notes related to XGBoost:*\n",
    "\n",
    "* DMatrix is a data structure used by XGBoost that is optimized for both memory efficiency and training speed. We will convert our training, validation, and test datasets into DMatrix format before training the model.\n",
    "\n",
    "* The param dictionary contains the parameters for the XGBoost model. eta is the learning rate, max_depth is the maximum depth of the trees, objective is the loss function to be minimized, and random_state is a seed for the random number generator for reproducibility.\n",
    "\n",
    "* num_round is the number of rounds of training, equivalent to the number of trees in the model.\n",
    "\n",
    "* The train function trains the model, and the predict function generates predictions. The predictions are probabilities of the positive class (fraudulent transactions), so they are between 0 and 1. We can convert these to class labels (0 or 1) by rounding them to the nearest integer (in reality, we could choose a different threshold depending on the business requirements).\n",
    "\n",
    "*Notes related to training and tuning with Vertex AI Vizier:*\n",
    "\n",
    "* We use the [cloudml-hypertune](https://github.com/GoogleCloudPlatform/cloudml-hypertune) Python package to pass metrics to Vertex AI. To learn more about this process, see the Google Cloud documentation [here](https://cloud.google.com/vertex-ai/docs/training/code-requirements#hp-tuning-metric).\n",
    "\n",
    "* For hyperparameter tuning, Vertex AI runs our training code multiple times, with different command-line arguments each time. Our training code must parse these command-line arguments and use them as hyperparameters for training.. To learn more about this process, see the Google Cloud documentation [here](https://cloud.google.com/vertex-ai/docs/training/code-requirements#command-line-arguments).\n",
    "\n",
    "**IMPORTANT:** Replace **YOUR_BUCKET_NAME** with your bucket name. \n",
    "This is because *writefile* will write the contents of this cell directly to file; it will not parse variables from earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c66db-6f72-4a9b-9596-b0187c396153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_DIR}/train.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.cloud import storage\n",
    "from hypertune import HyperTune\n",
    "\n",
    "data_location='gs://YOUR_BUCKET_NAME/creditcard.csv'\n",
    "\n",
    "def train_model(data, max_depth, eta, gamma):\n",
    "    X = data.iloc[:,:-1]\n",
    "    y = data.iloc[:,-1]\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Split the non-training data into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "        \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        'max_depth': max_depth,\n",
    "        'eta': eta,\n",
    "        'gamma': gamma,\n",
    "        'objective': 'binary:logistic',\n",
    "        'nthread': 4,\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    evallist = [(dval, 'eval')]\n",
    "\n",
    "    num_round = 10\n",
    "    model = xgb.train(params, dtrain, num_round, evallist)\n",
    "    \n",
    "    preds = model.predict(dtest)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    hpt = HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='auc',\n",
    "        metric_value=auc,\n",
    "        global_step=1000)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='XGBoost Hyperparameter Tuning')\n",
    "    parser.add_argument('--max_depth', type=int, default=3)\n",
    "    parser.add_argument('--eta', type=float, default=0.3)\n",
    "    parser.add_argument('--gamma', type=float, default=0)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    data = pd.read_csv(data_location)\n",
    "    model = train_model(data, args.max_depth, args.eta, args.gamma)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8050141-8b58-4566-8d1d-5b42c9a836b1",
   "metadata": {},
   "source": [
    "### Create our requirements.txt file\n",
    "The requirements.txt file is a convenient way to specify all of the packages that we want to install in our custom container image. This file will be referenced in the Dockerfile for our image.\n",
    "\n",
    "In this case, we will install:\n",
    "* [XGBoost](https://xgboost.readthedocs.io/en/stable/)\n",
    "* [cloudml-hypertune 0.1.0.dev6](https://pypi.org/project/cloudml-hypertune/)\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Python Client for Google Cloud Storage](https://cloud.google.com/python/docs/reference/storage/latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e733c1-8987-40bb-9e32-319af45361ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {APPLICATION_DIR}/requirements.txt\n",
    "xgboost\n",
    "cloudml-hypertune==0.1.0.dev6\n",
    "google-cloud-aiplatform\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92bd89-640c-4d41-a20f-5f2c18262904",
   "metadata": {},
   "source": [
    "### Create the Dockerfile for our custom training container\n",
    "\n",
    "The [Dockerfile](https://docs.docker.com/engine/reference/builder/) specifies how to build our custom container image.\n",
    "\n",
    "This Dockerfile specifies that we want to:\n",
    "1. Use a Vertex AI [prebuilt container for custom training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) as a base image.\n",
    "2. Install the required dependencied specified in our requirements.txt file.\n",
    "3. Copy our custom training script to the container image.\n",
    "4. Run our custom training script when the container starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381653d-be05-449b-aa8b-7241e1791027",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {APPLICATION_DIR}/Dockerfile\n",
    "\n",
    "# Use a Vertex AI prebuilt container for custom training as a base image.\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest\n",
    "\n",
    "# Specify the working directory to use in our container\n",
    "WORKDIR /\n",
    "\n",
    "# Copy our requirements.txt file to our container image\n",
    "COPY requirements.txt /requirements.txt\n",
    "\n",
    "# Install the packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the training code to our container image\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke our training code.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32ae9c-7328-4e48-b8af-3e3ad992d514",
   "metadata": {},
   "source": [
    "### Build the container and put it in Google Artifact Registry\n",
    "Next, we'll create a Docker repository in Google Artifact Registry, build our container, and push it to the newly-created repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8229fa1-2076-4fab-9744-8705ca703397",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME=f'{APP_NAME}-app'\n",
    "\n",
    "!gcloud artifacts repositories create $REPO_NAME --repository-format=docker \\\n",
    "--location=$REGION --description=\"Docker repository\"\n",
    "\n",
    "! gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cd6f6-48dd-4f12-ad5e-66b565574d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URI = (\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{APP_NAME}:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c288bac-0f0e-4609-afc9-cf826e727b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $APPLICATION_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cc0fd-e96d-4fd7-bad8-c222b7cbf664",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build ./ -t $IMAGE_URI --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3ad35-0242-4fc6-a43b-0f2508abeb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417826a-11dd-46fb-941b-cc45d926db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42785616-dddf-40cd-beb3-b111f0d24dfa",
   "metadata": {},
   "source": [
    "## Configure a hyperparameter tuning job\n",
    "Now that our training application code is containerized, it's time to specify and run the hyperparameter tuning job.\n",
    "\n",
    "To create the hyperparameter tuning job, we need to first define the worker_pool_specs, which specifies the machine type and Docker image to use. The following spec includes one n1-standard-4 machine. (For more details, see the Google Cloud documentation [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#WorkerPoolSpec).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec3c29-c5d5-4b3d-a75b-75c6e6a45408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The spec for the worker pools, including machine type and Docker image\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36cc21-9a99-4783-bad0-d94da3739622",
   "metadata": {},
   "source": [
    "## Define our custom job spec and hyperparameter tuning spec.\n",
    "Next, we define our custom job spec (referencing the worker pool specs we just created), and our hyperparameter tuning spec, which includes details such as the hyperparameters and the metrics we want to optimize. (For more details, see the Google Cloud documentation [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#aiplatform_create_hyperparameter_tuning_job_python_package_sample-python).)\n",
    "\n",
    "**IMPORTANT:** If you named your service account anything other than **ai-ml-sa** when you created it at the beginning of Chapter 8 then you will need to replace it in this code cell. (If you followed the recommended naming then you do not need to make a change here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b013de7-8b55-4e1f-98c3-5018ba011ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom job\n",
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=\"xgboost_train\",\n",
    "    worker_pool_specs=worker_pool_specs\n",
    ")\n",
    "\n",
    "# Specify service account\n",
    "service_account_email = f\"ai-ml-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "\n",
    "# Set the custom service account in the job config\n",
    "custom_job.service_account_email = service_account_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778e182-acac-47e3-865f-37e338234676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter tuning spec\n",
    "hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=\"xgboost_hpt\",\n",
    "    custom_job=custom_job,\n",
    "    metric_spec={\n",
    "        \"auc\": \"maximize\",\n",
    "    },\n",
    "    parameter_spec={\n",
    "        \"eta\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.01, max=0.3, scale='unit'),\n",
    "        \"max_depth\": aiplatform.hyperparameter_tuning.IntegerParameterSpec(min=3, max=10, scale='unit'),\n",
    "        \"gamma\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0, max=1, scale='unit'),\n",
    "    },\n",
    "    max_trial_count=20,\n",
    "    parallel_trial_count=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3439a-84af-4871-8552-481280a80e8b",
   "metadata": {},
   "source": [
    "# Run the hyperparameter tuning job\n",
    "\n",
    "The following cell will run our job. Considering that the tuning job will include many trials, it may run for a long time (perhaps an hour or two). The output of this cell will display a link that will enable you to view the status of the tuning job in the Google Cloud console. The output of this cell will also repetitively display the current status of the tuning job every few seconds here in this notebook. Wait until the current status says \"JOB_STATE_SUCCEEDED HyperparameterTuningJob run completed\", and then we will inspect the optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3774b-5abf-41f8-a6ff-c839eec08d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter tuning job\n",
    "hpt_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710f7eb-a920-45c0-a567-65980509c028",
   "metadata": {},
   "source": [
    "# Extract the best hyperparameters\n",
    "\n",
    "In the next cell, we will get a list of all of the trials from our tuning job, then find the best-performing trial, and extract its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084a649-049c-4173-b5fb-6b38a5d48379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of trials sorted by the objective metric (auc) in descending order\n",
    "trials = sorted(hpt_job.trials, key=lambda trial: trial.final_measurement.metrics[0].value, reverse=True)\n",
    "\n",
    "# The first trial in the sorted list is the best trial\n",
    "best_trial = trials[0]\n",
    "best_auc = trials[0].final_measurement.metrics[0].value\n",
    "\n",
    "# Extract hyperparameters of the best trial\n",
    "best_hyperparameters = best_trial.parameters\n",
    "\n",
    "print(f\"Best AUC: {best_auc}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87223a-ffcb-4367-a0a7-17a004fbd537",
   "metadata": {},
   "source": [
    "# Train a model with the best hyperparameters\n",
    "\n",
    "Now, let's train a model with the best hyperparameters that were produced by our tuning job.\n",
    "\n",
    "Note that **num_boost_round** is not a parameter of the model, but rather a parameter of the training function, so we will handle it separately. We will also convert it to an integer type here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ceeb2-cd5c-4b6c-beda-12e91f1a50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "for param in best_hyperparameters:\n",
    "    param_id = param.parameter_id\n",
    "    if param_id == 'num_boost_round':\n",
    "        best_params[param_id] = int(param.value)\n",
    "    else:\n",
    "        best_params[param_id] = param.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13956b3-4fb4-4d5d-8cec-9a2683da3aa1",
   "metadata": {},
   "source": [
    "## Install XGboost\n",
    "\n",
    "Let's install XGBoost so we can train a model directly here in our notebook (remember that our previous training jobs happened in a Docker container that we had created.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47da29d-5c0e-456f-a5ea-03ab0fccc3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0250d54-8680-4902-949d-554a282059c2",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We will use a modified version of our earlier training code. In this case, we will directly provide the \"best_params\" to the training job.\n",
    "\n",
    "The ouput of this cell will show us the ROC-AUC score achieved against the **validation** dataset for each training round (specified by num_round).\n",
    "\n",
    "Finally, we will evaluate our model against the **test** dataset, and print the resulting ROC-AUC score for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ade40f-3720-4efa-a700-2fbd8809bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_location=f'{BUCKET_URI}/creditcard.csv'\n",
    "\n",
    "def train_model(data, hyperparameters):\n",
    "    X = data.iloc[:,:-1]\n",
    "    y = data.iloc[:,-1]\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Split the non-training data into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "        \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Convert max_depth to int (xgboost expects it as an int)\n",
    "    hyperparameters['max_depth'] = int(hyperparameters['max_depth'])\n",
    "\n",
    "    hyperparameters.update({\n",
    "        'objective': 'binary:logistic',\n",
    "        'nthread': 4,\n",
    "        'eval_metric': 'auc'\n",
    "    })\n",
    "    \n",
    "    evallist = [(dval, 'eval')]\n",
    "\n",
    "    num_round = 10\n",
    "    model = xgb.train(hyperparameters, dtrain, num_round, evals=evallist)\n",
    "    \n",
    "    preds = model.predict(dtest)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    print(f'ROC-AUC Score on Test Set: {auc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv(data_location)\n",
    "    model = train_model(data, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde69502-86be-4ba8-b190-2a616189ae81",
   "metadata": {},
   "source": [
    "**Note:** when I ran this, I got an ROC-AUC score of 0.9188, which is pretty good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
