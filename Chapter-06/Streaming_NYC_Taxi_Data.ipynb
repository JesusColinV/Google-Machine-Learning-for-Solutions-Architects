{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Apache Beam on Google Cloud Dataflow to process streaming data\n",
    "\n",
    "This example demonstrates how to set up a streaming pipeline that processes a stream that contains NYC taxi ride data.\n",
    "\n",
    "It's a modified version of an example from Google's example repository, and it reads the data from a public Google Cloud Pub/Sub topic that Google created for people to be able to test streaming data processing use-cases.\n",
    "\n",
    "Each element in the stream contains the location of the taxi, the timestamp, the meter reading, the meter increment, the passenger count, and ride status in JSON format.\n",
    "\n",
    "We will use Apache Beam to define the pipeline, run it on Google Cloud Dataflow, and store the results in Google Cloud BigQuery.\n",
    "To get a better understanding of the Apache Beam constructs we use in this example, see [Apache Beam Basics](https://beam.apache.org/documentation/basics/), and the [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "import google.auth\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is only needed in order to avoid pip version warnings later in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: pip in /jupyter/.kernels/apache-beam-2.46.0/lib/python3.8/site-packages (23.1.2)\n"
     ]
    }
   ],
   "source": [
    "!/jupyter/.kernels/apache-beam-2.46.0/bin/python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the Google Cloud Pub/Sub topic that we will read from, and the Google Cloud Storage bucket that Dataflow will use to store the data in transit. \n",
    "\n",
    "**IMPORTANT:\n",
    "Replace GCS-BUCKET-NAME with your own GCS bucket name (you created a GCS bucket earlier in this book).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pub/Sub source topic\n",
    "topic = \"projects/pubsub-public-data/topics/taxirides-realtime\"\n",
    "\n",
    "# Google Cloud Storage location.\n",
    "dataflow_gcs_location = 'gs://GCS-BUCKET-NAME/dataflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the options to create the streaming pipeline.\n",
    "For more information, see [Apache Beam Pipeline Options](https://beam.apache.org/releases/pydoc/2.4.0/apache_beam.options.pipeline_options.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Beam pipeline options.\n",
    "options = beam.options.pipeline_options.PipelineOptions(flags={})\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(beam.options.pipeline_options.StandardOptions).streaming = True\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "# The project will be used for creating a subscription to the PubSub topic.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline with the options we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a `PTransform` that will create a subscription to the given Pub/Sub topic and reads from the subscription. \n",
    "The data is in JSON format, so we add another `Map` `PTransform` to parse the data as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = p | \"read\" >> beam.io.ReadFromPubSub(topic=topic) | beam.Map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are reading from an unbounded source, we need to create a windowing scheme.\n",
    "We will use sliding windows with a 10-second duration for each window, and with one second for each slide.\n",
    "For more information about windowing in Apache Beam, see [Windowing Basics](https://beam.apache.org/documentation/programming-guide/#windowing-basics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_data = (data | \"window\" >> beam.WindowInto(beam.window.SlidingWindows(10, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there will be some duplicate data for each element because with sliding windows each element has to appear in multiple\n",
    "windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the 10-second dollar run rate for each second, by summing the `meter_increment` JSON field for each window.\n",
    "\n",
    "First, extract the `meter_increment` field from the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_increments = windowed_data | beam.Map(lambda e: e.get('meter_increment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sum all elements by window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rates = meter_increments | beam.CombineGlobally(sum).without_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our pipeline defined. Next, we will specify some additional options in order to run this pipeline on Google Cloud Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Google Cloud region to run Dataflow.\n",
    "options.view_as(GoogleCloudOptions).region = 'us-central1'\n",
    "\n",
    "# Set the staging location. This location is used to stage the\n",
    "# Dataflow pipeline and SDK binary.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Set the temporary location. This location is used to store temporary files\n",
    "# or intermediate results before outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the schema for our BigQuery Table. Dataflow will create the table in BigQuery, with this schema. The outputs are just the aggregated run_rates, so our schema consists of just one field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema = {\n",
    "    'fields': [{\n",
    "        'name': 'run_rates', 'type': 'NUMERIC', 'mode': 'NULLABLE'\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in the process is to write the results (i.e., the contents of the run_rates variable we created above) to BigQuery. However, the run_rates variable currently just contains numeric data, whereas the WriteToBigQuery transform expects a dictionary with a key-value pair that matches the schema. We also want to ensure that we don't exceed the size of BigQuery's NUMERIC datatype, so we round the rate value to 5 decimal places just to be safe. The following code will perform the necessary conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert run_rates to a dictionary\n",
    "def to_dict(rate):\n",
    "    return {'run_rates': round(rate, 5)}\n",
    "\n",
    "run_rates_dict = run_rates | 'ConvertToDict' >> beam.Map(to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready to be written to BigQuery, and the following code will create the BigQuery sink for that purpose.\n",
    "**IMPORTANT:\n",
    "Replace GOOGLE-CLOUD-PROJECT with the name of your Google Cloud project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.io.gcp.bigquery.WriteResult at 0x7f55fb1a0790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write results to BigQuery\n",
    "run_rates_dict | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(\n",
    "    table='GOOGLE-CLOUD-PROJECT.taxirides.run_rates_table',\n",
    "    schema=table_schema,\n",
    "    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have defined all of the steps in our data processing pipeline, and now we will run the pipeline on Google Cloud Dataflow. After running this cell, it will display details regarding the pipeline result, and you can go to the [Dataflow Jobs Console](https://console.cloud.google.com/dataflow/jobs/) to view the additional job details and execution status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DataflowPipelineResult <Job\n",
       " clientRequestId: '20230507205544023415-5911'\n",
       " createTime: '2023-05-07T20:55:46.191755Z'\n",
       " currentStateTime: '1970-01-01T00:00:00Z'\n",
       " id: '2023-05-07_13_55_45-17532039298532056386'\n",
       " location: 'us-central1'\n",
       " name: 'beamapp-root-0507205544-022088-qxs2zhy3'\n",
       " projectId: 'still-sight-352221'\n",
       " stageStates: []\n",
       " startTime: '2023-05-07T20:55:46.191755Z'\n",
       " steps: []\n",
       " tempFiles: []\n",
       " type: TypeValueValuesEnum(JOB_TYPE_STREAMING, 2)> at 0x7f55fae7bdc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner = beam.runners.DataflowRunner()\n",
    "runner.run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.46.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.46.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
