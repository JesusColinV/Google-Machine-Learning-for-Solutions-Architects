{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# Introduction and setup\n",
    "\n",
    "In this notebook, we will build a pipeline that will perform the following steps:\n",
    "1. Custom data processing such as feature scaling, one-hot encoding, and feature engineering, in a Google Cloud Serverless Spark environment.\n",
    "2. Implement a custom training job in Vertex AI to train a custom model. In this case, our model uses the [Titanic dataset from Kaggle](https://www.kaggle.com/competitions/titanic/data) to predict the likelihood of survival of each passenger based on their associated features in the dataset.\n",
    "3. Upload the trained model to Vertex AI Model Registry.\n",
    "4. Deploy the trained model to a Vertex AI endpoint for online inference.\n",
    "\n",
    "In this initial section, we set up all of the baseline requirements to run our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Install required packages\n",
    "\n",
    "Install additional package dependencies not installed in your notebook environment, such as Pyspark, MLeap and others. Use the latest major GA version of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --quiet --user --upgrade google-cloud-aiplatform kfp google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Import required libraries\n",
    "\n",
    "We will use the following libraries in this notebook:\n",
    "\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Kubeflow Pipelines (KFP)](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/sdk-overview/)\n",
    "* [Google Cloud Pipeline Components (GCPC)](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines (KFP)\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component, Input, Output, Artifact\n",
    "\n",
    "# Google Cloud Pipeline Components (GCPC)\n",
    "from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp\n",
    "from google_cloud_pipeline_components.v1 import dataset, custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants\n",
    "In this section, we define all of the constants that will be referenced throughout the rest of the notebook.\n",
    "\n",
    "**REPLACE THE PROJECT_ID, REGION, AND BUCKET DETAILS WITH YOUR DETAILS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gQOV9ssPWbMB"
   },
   "outputs": [],
   "source": [
    "# Core constants\n",
    "PROJECT_ID=\"YOUR_PROJECT_ID\"\n",
    "REGION=\"YOUR_REGION\"\n",
    "BUCKET=\"YOUR_BUCKET\"\n",
    "BUCKET_URI = f\"gs://{BUCKET}\"\n",
    "APPLICATION_DIR = \"mlops-titanic-app\" # Local parent directory for our pipeline resources\n",
    "TRAINER_DIR = f\"{APPLICATION_DIR}/trainer\" # Local directory for training resources\n",
    "PYSPARK_DIR = \"pyspark-titanic-dir\" # Local directory for PySpark data processing resources\n",
    "APP_NAME=\"mlops-titanic\" # Base name for our pipeline application\n",
    "\n",
    "# Pipeline constants\n",
    "PIPELINE_NAME = \"mlops-titanic-pipeline\" # Name of our pipeline\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\" # (See: https://www.kubeflow.org/docs/components/pipelines/v1/overview/pipeline-root/)\n",
    "SUBNETWORK = \"default\" # Our VPC subnet name\n",
    "SUBNETWORK_URI = f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{SUBNETWORK}\" # Our VPC subnet resource identifier\n",
    "MODEL_NAME = \"mlops-titanic\" # Name of our model\n",
    "EXPERIMENT_NAME = \"aiml-sa-mlops-experiment\" # Vertex AI \"Experiment\" name for metadata tracking\n",
    "\n",
    "# Preprocessing constants\n",
    "PYSPARK_REPO_NAME=f'{APP_NAME}-pyspark' # Name of repository in which we will store our custom PySpark image\n",
    "PYSPARK_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{PYSPARK_REPO_NAME}/{APP_NAME}-pyspark:latest\"\n",
    "SOURCE_DATASET = f\"{BUCKET_URI}/data/unprocessed/titanic/train.csv\" # Our raw source dataset\n",
    "PREPROCESSING_PYTHON_FILE_URI = f\"{BUCKET_URI}/code/mlops/preprocessing.py\" # GCS location of our PySpark script\n",
    "PROCESSED_DATA_URI =f\"{BUCKET_URI}/data/processed/mlops-titanic\" # Location to store the output of our data preprocessing step\n",
    "DATAPROC_RUNTIME_VERSION = \"2.1\" # (See https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions)\n",
    "# Arguments to pass to our preprocessing script:\n",
    "PREPROCESSING_ARGS = [\n",
    "    \"--source_dataset\",\n",
    "    SOURCE_DATASET,\n",
    "    \"--processed_data_path\",\n",
    "    PROCESSED_DATA_URI,\n",
    "]\n",
    "\n",
    "# Training constants\n",
    "TRAIN_REPO_NAME=f'{APP_NAME}-train' # Name of repository in which we will store our custom training image\n",
    "TRAIN_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{TRAIN_REPO_NAME}/{APP_NAME}-train:latest\"\n",
    "MODEL_URI = f\"{BUCKET_URI}/models/mlops-chapter/titanic\" # Where to store our trained model\n",
    "# Where to store our test data:\n",
    "TEST_DATA_PREFIX = \"test_data\" \n",
    "TEST_DATA_DIR = f\"{TEST_DATA_PREFIX}_dir\"\n",
    "TEST_DATA_FILE_NAME = f\"{TEST_DATA_PREFIX}.jsonl\"\n",
    "TEST_DATASET_PATH = f\"{BUCKET_URI}/{TEST_DATA_FILE_NAME}\"\n",
    "LOCAL_TEST_DATASET_PATH = f\"./{TEST_DATA_DIR}/{TEST_DATA_FILE_NAME}\"\n",
    "\n",
    "# Hyperparameters for training\n",
    "BATCH_SIZE: int = 32\n",
    "EPOCHS: int = 20\n",
    "LEARNING_RATE: float = 0.001\n",
    "N_HIDDEN_LAYERS: int = 3\n",
    "N_UNITS: int = 64\n",
    "ACTIVATION_FN: str = 'relu'\n",
    "\n",
    "# Arguments to pass to our training job\n",
    "TRAINING_ARGS=[\n",
    "        \"--project_id\",\n",
    "        PROJECT_ID,\n",
    "        \"--bucket_name\",\n",
    "        BUCKET,\n",
    "        \"--processed_data_path\",\n",
    "        PROCESSED_DATA_URI,\n",
    "        \"--test_data_file_name\",\n",
    "        TEST_DATA_FILE_NAME,\n",
    "        \"--model_path\",\n",
    "        MODEL_URI,\n",
    "        \"--batch_size\",\n",
    "        str(BATCH_SIZE),\n",
    "        \"--epochs\",\n",
    "        str(EPOCHS),\n",
    "        \"--learning_rate\",\n",
    "        str(LEARNING_RATE),\n",
    "        \"--n_hidden_layers\",\n",
    "        str(N_HIDDEN_LAYERS),\n",
    "        \"--n_units\",\n",
    "        str(N_UNITS),\n",
    "        \"--activation_fn\",\n",
    "        ACTIVATION_FN,\n",
    "    ]\n",
    "\n",
    "# Worker pool spec (see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#workerpoolspec)\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE_URI,\n",
    "            \"args\": TRAINING_ARGS\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Serving constants\n",
    "SERVING_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\" # (See: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)\n",
    "ENDPOINT_NAME = \"mlops-endpoint\" # Name of endpoint on which to serve our trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local directories\n",
    "We will use the following local directories during the activities in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a source directory to save the code\n",
    "!mkdir -p $APPLICATION_DIR\n",
    "!mkdir -p $TRAINER_DIR\n",
    "!mkdir -p $PYSPARK_DIR\n",
    "!mkdir -p $TEST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload source dataset \n",
    "Upload our source dataset to GCS. Our data preprocessing step in our pipeline will ingest this data from GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./data/train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 59.8 KiB/ 59.8 KiB]                                                \n",
      "Operation completed over 1 objects/59.8 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp ./data/train.csv $SOURCE_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set  project ID for  gcloud\n",
    "The following command sets our project ID for using gcloud commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vertex AI SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Private Google Access for Dataproc \n",
    "Our Serverless Spark data preprocessing job in our pipeline will run in Dataproc, which is (as Google defines) Google's \"fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks.\"\n",
    "We're going to configure something called \"Private Google Access\", which allows us to interact with Google services without sending requests over the public Internet.\n",
    "\n",
    "You can learn more about Dataproc [here](https://cloud.google.com/dataproc?hl=en), and learn more about Private Google Access [here](https://cloud.google.com/vpc/docs/private-google-access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     REGION       NETWORK  RANGE          STACK_TYPE  IPV6_ACCESS_TYPE  INTERNAL_IPV6_PREFIX  EXTERNAL_IPV6_PREFIX\n",
      "default  us-central1  default  10.128.0.0/20  IPV4_ONLY\n",
      "Updated [https://www.googleapis.com/compute/v1/projects/still-sight-352221/regions/us-central1/subnetworks/default].\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks subnets list --regions=$REGION --filter=$SUBNETWORK\n",
    "\n",
    "!gcloud compute networks subnets update $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--enable-private-ip-google-access\n",
    "\n",
    "!gcloud compute networks subnets describe $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Google Artifact Registry repositories\n",
    "\n",
    "Our custom preprocessing and custom training components in our pipeline will run in containers on Dataproc Serverless Spark and Vertex AI Training, respectively. In this sestion, we will create the Google Artifact Registry repositories in which we can store our custom container images that we will build in later steps in this notebook.\n",
    "\n",
    "Our data preprocessing step in our pipeline will use Serverless Spark to perform our data preparation and feature engineering steps. We will use our own PySpark code to perform the data processing steps, so we're going to create a custom container to run our code.\n",
    "This step creates a Docker repository in Google Artifact Registry, which is where we will store our custom container image that we're going to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Google Artifact Registry repository for our custom preprocessing container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [mlops-titanic-pyspark]\n",
      "Waiting for operation [projects/still-sight-352221/locations/us-central1/operat\n",
      "ions/6456da31-d277-4975-bfe4-2fed546062f5] to complete...done.                 \n",
      "Created repository [mlops-titanic-pyspark].\n",
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create $PYSPARK_REPO_NAME --repository-format=docker \\\n",
    "--location=$REGION --description=\"PySpark repo for MLOps workload\"\n",
    "\n",
    "# Register gcloud as a Docker credential helper\n",
    "! gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z949NOrk-WXQ"
   },
   "source": [
    "#### Create Google Artifact Registry repository for our custom training container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cnj-_0Ljzox_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [mlops-titanic-train]\n",
      "Waiting for operation [projects/still-sight-352221/locations/us-central1/operat\n",
      "ions/4e7f375f-b13a-42ff-9ccb-06c5ee9b1007] to complete...done.                 \n",
      "Created repository [mlops-titanic-train].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create $TRAIN_REPO_NAME --repository-format=docker \\\n",
    "--location=$REGION --description=\"Train repo for MLOps workload\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom PySpark job\n",
    "In this section, we will create our custom PySpark job. It will consist of the following steps:\n",
    "1. Create our custom Pyspark script.\n",
    "2. Create a Dockerfile that will specify how to build our custom container image. See [here](https://docs.docker.com/engine/reference/builder/) for more details. \n",
    "3. Build our custom container image.\n",
    "4. Push our custom container image to Google Artifact Registry so that we can use it in subsequent steps in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac5229530e16"
   },
   "source": [
    "## Define the code for our PySpark job\n",
    "\n",
    "The following code will create a file that contains the code for our custom PySpark data preprocessing job. \n",
    "\n",
    "The code initiates a Spark session, loads our raw source dataset, and then performs the following processing steps (we performed many of these steps using pandas in our feature engineering chapter earlier in this book, but in this case we will implement the steps using PySpark in Google Cloud Serverless Spark):\n",
    "\n",
    "1. Removes rows from the dataset where the target variable (\"Survived\") is missing values.\n",
    "2. Drops columns that are unlikely to affect the likelihod of surviving, such as 'PassengerId', 'Name', 'Ticket', and 'Cabin'.\n",
    "3. Fills in missing values in input features.\n",
    "4. Performs some feature engineering by creating new features such as 'FamilySize' and 'IsAlone' from combinations of existing features.\n",
    "5. Ensures that all numeric features are on a consistent scale with each other.\n",
    "6. One-hot encodes all categorical features.\n",
    "7. Converts the resulting sparse vector to a dense vector. This mainly makes it easier for us to feed the data into our Keras model in our training script later, with minimal processing needed in the training script.\n",
    "8. Writes the resulting processed data to a parquet file in GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyspark-titanic-dir/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $PYSPARK_DIR/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
    "\n",
    "# Setting up the argument parser\n",
    "parser = argparse.ArgumentParser(description='Data Preprocessing Script')\n",
    "parser.add_argument('--source_dataset', type=str, help='Path to the source dataset')\n",
    "parser.add_argument('--processed_data_path', type=str, help='Path to save the output data')\n",
    "\n",
    "# Parsing the arguments\n",
    "args = parser.parse_args()\n",
    "source_dataset = args.source_dataset\n",
    "processed_data_path = args.processed_data_path\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "titanic = spark.read.csv(args.source_dataset, header=True, inferSchema=True)\n",
    "\n",
    "# Remove rows where 'Survived' is missing\n",
    "titanic = titanic.filter(titanic.Survived.isNotNull())\n",
    "\n",
    "# Drop irrelevant columns\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket', 'Cabin')\n",
    "\n",
    "# Fill missing values\n",
    "def calculate_median(column_name):\n",
    "    return titanic.filter(col(column_name).isNotNull()).approxQuantile(column_name, [0.5], 0)[0]\n",
    "\n",
    "median_age = calculate_median('Age')  # Median age\n",
    "median_fare = calculate_median('Fare')  # Median fare\n",
    "\n",
    "titanic = titanic.fillna({\n",
    "    'Pclass': -1,\n",
    "    'Sex': 'Unknown',\n",
    "    'Age': median_age,\n",
    "    'SibSp': -1,\n",
    "    'Parch': -1,\n",
    "    'Fare': median_fare,\n",
    "    'Embarked': 'Unknown'\n",
    "})\n",
    "\n",
    "# Feature Engineering\n",
    "titanic = titanic.withColumn('FamilySize', col('SibSp') + col('Parch') + 1)\n",
    "titanic = titanic.withColumn('IsAlone', when(col('FamilySize') == 1, 1).otherwise(0))\n",
    "\n",
    "# Define categorical features \n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'IsAlone']\n",
    "\n",
    "# Define numerical features \n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize']\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "stages = []\n",
    "for col_name in categorical_features:\n",
    "    string_indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[f\"{col_name}_Index\"], outputCols=[f\"{col_name}_Vec\"])\n",
    "    stages += [string_indexer, encoder]\n",
    "    \n",
    "# Scaling numerical features \n",
    "for col_name in numerical_features:\n",
    "    assembler = VectorAssembler(inputCols=[col_name], outputCol=f\"vec_{col_name}\")\n",
    "    scaler = StandardScaler(inputCol=f\"vec_{col_name}\", outputCol=f\"scaled_{col_name}\", withStd=True, withMean=False)\n",
    "    stages += [assembler, scaler]\n",
    "\n",
    "# Create a pipeline and transform the data\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline_model = pipeline.fit(titanic)\n",
    "titanic = pipeline_model.transform(titanic)\n",
    "\n",
    "# Drop intermediate columns created during scaling and one-hot encoding\n",
    "titanic = titanic.drop('vec_Age', 'vec_Fare', 'vec_FamilySize', 'vec_SibSp', 'vec_Parch', 'Pclass_Index', 'Sex_Index', 'Embarked_Index', 'IsAlone_Index')\n",
    "\n",
    "# Drop original categorical columns (no longer needed after one-hot encoding)\n",
    "titanic = titanic.drop(*categorical_features)\n",
    "\n",
    "# Drop original numeric columns (no longer needed after scaling)\n",
    "titanic = titanic.drop(*numerical_features)\n",
    "\n",
    "vector_columns = [\"Pclass_Vec\", \"Sex_Vec\", \"Embarked_Vec\", \"IsAlone_Vec\", \"scaled_Age\", \"scaled_Fare\", \"scaled_FamilySize\", \"scaled_SibSp\", \"scaled_Parch\"]\n",
    "\n",
    "def to_dense(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "to_dense_udf = udf(to_dense, ArrayType(FloatType()))\n",
    "\n",
    "for vector_col in vector_columns:\n",
    "    titanic = titanic.withColumn(vector_col, to_dense_udf(col(vector_col)))\n",
    "\n",
    "for vector_col in vector_columns:\n",
    "    num_features = len(titanic.select(vector_col).first()[0])  # Getting the size of the vector\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        titanic = titanic.withColumn(f\"{vector_col}_{i}\", col(vector_col).getItem(i))\n",
    "    \n",
    "    titanic = titanic.drop(vector_col)\n",
    "\n",
    "# Save the processed data to GCS\n",
    "titanic.write.parquet(args.processed_data_path, mode=\"overwrite\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dockerfile for our PySpark container\n",
    "\n",
    "The [Dockerfile](https://docs.docker.com/engine/reference/builder/) specifies how to build our custom container image.\n",
    "\n",
    "This Dockerfile (adapted from [this official Google example](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_dataproc_tabular.ipynb)) specifies that we want to:\n",
    "1. Use a base Debian image.\n",
    "2. Install required dependencied such as [procps](https://www.linux.co.cr/ldp/lfs/appendixa/procps.html), [tini](https://packages.debian.org/sid/tini), and [openjdk-11-jdk-headless](https://packages.debian.org/sid/openjdk-11-jdk-headless).\n",
    "3. Set required environment variables.\n",
    "4. Copy our PySpark script to the container image.\n",
    "5. Set required permissions.\n",
    "6. Run our PySpark script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyspark-titanic-dir/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYSPARK_DIR}/Dockerfile\n",
    "\n",
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Debian 11 is recommended.\n",
    "FROM debian:11-slim\n",
    "\n",
    "# Suppress interactive prompts\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Switch to the root user for installation\n",
    "USER root\n",
    "\n",
    "# (Required) Install utilities required by Spark scripts.\n",
    "RUN apt update && apt install -y procps tini\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update -y && \\\n",
    "    apt-get install -y wget openjdk-11-jdk-headless && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Set environment variables for Java\n",
    "ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64/\n",
    "RUN export JAVA_HOME\n",
    "\n",
    "# Install Spark\n",
    "ENV APACHE_SPARK_VERSION 3.1.2\n",
    "RUN wget -qO - https://archive.apache.org/dist/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-hadoop3.2.tgz | tar -xz -C /usr/local/ && \\\n",
    "    ln -s /usr/local/spark-${APACHE_SPARK_VERSION}-bin-hadoop3.2 /usr/local/spark\n",
    "\n",
    "ENV SPARK_HOME /usr/local/spark\n",
    "ENV PATH $PATH:/usr/local/spark/bin\n",
    "\n",
    "COPY preprocessing.py /preprocessing.py\n",
    "\n",
    "# (Required) Create the 'spark' group/user.\n",
    "# The GID and UID must be 1099. Home directory is required.\n",
    "RUN groupadd -g 1099 spark\n",
    "RUN useradd -u 1099 -g 1099 -d /home/spark -m spark\n",
    "USER spark\n",
    "\n",
    "CMD [\"spark-submit\", \"/preprocessing.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a custom container image for our PySpark job\n",
    "\n",
    "Our custom PySpark job will run on Google Cloud Serverless Spark, which executes our data processing code as a Batch job on [Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs). This requires our code to be packaged into a Docker container, and that's what we're doing in this section. Our custom container image will then be referenced in the `DataprocPySparkBatchOp` component in our pipeline to run the Serverless Spark job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we change our working directory to the local PySpark directory that we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Chapter-11-MLops/pyspark-titanic-dir\n"
     ]
    }
   ],
   "source": [
    "cd $PYSPARK_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build our custom container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:6335b55851a6888db1c3510f96b5eb3456866075ea0efef762c68767708b2ea1\n"
     ]
    }
   ],
   "source": [
    "! docker build ./ -t $PYSPARK_IMAGE_URI --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we push our image to Google Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/still-sight-352221/mlops-titanic-pyspark/mlops-titanic-pyspark]\n",
      "\n",
      "\u001b[1Bcdda1b6d: Preparing \n",
      "\u001b[1B3e1bd22b: Preparing \n",
      "\u001b[1Bf3e38a64: Preparing \n",
      "\u001b[1B914ac522: Preparing \n",
      "\u001b[1Bed1d026e: Preparing \n",
      "\u001b[1Bd1067a00: Preparing \n",
      "\u001b[2Bd1067a00: Mounted from still-sight-352221/mlops-titanic-train-pyspark/mlops-titanic-train-pyspark \u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:d380b67ac9b7c278fac3588d7994ab16f3d65b0b33396d80111da3f49fa9c2f2 size: 1790\n"
     ]
    }
   ],
   "source": [
    "! docker push $PYSPARK_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change our working directory back to our original working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Chapter-11-MLops\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload source code for PySpark\n",
    "\n",
    "This is a required step for the `DataprocPySparkBatchOp` component in google-cloud-pipeline-components. We need to upload our PySpark code to our GCS bucket to be references by the `DataprocPySparkBatchOp` component in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://pyspark-titanic-dir/preprocessing.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  3.9 KiB/  3.9 KiB]                                                \n",
      "Operation completed over 1 objects/3.9 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp $PYSPARK_DIR/preprocessing.py $PREPROCESSING_PYTHON_FILE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom training job\n",
    "In this section, we will create our custom training job. It will consist of the following steps:\n",
    "1. Create our custom training script.\n",
    "2. Create a Dockerfile that will specify how to build our custom container image. \n",
    "3. Build our custom container image.\n",
    "4. Push our custom container image to Google Artifact Registry so that we can use it in subsequent steps in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "399eba3ab133"
   },
   "source": [
    "## Define the code for our training job\n",
    "\n",
    "The following code will create a file that contains the code for our custom training job. \n",
    "\n",
    "The code performs the following processing steps:\n",
    "\n",
    "1. Imports required libraries and sets initial variable values based on arguments passed to the script (the arguments are described below).\n",
    "2. Reads in the processed dataset that was created by the data preprocessing step in our pipeline.\n",
    "3. Fills in missing values in input features.\n",
    "4. Performs some feature engineering by creating new features such as 'FamilySize' and 'IsAlone' from combinations of existing features.\n",
    "5. Ensures that all numeric features are on a consistent scale with each other.\n",
    "6. One-hot encodes all categorical features.\n",
    "7. Converts the resulting sparse vector to a dense vector. This mainly makes it easier for us to feed the data into our Keras model in our training script later, with minimal processing needed in the training script.\n",
    "8. Writes the resulting processed data to a parquet file in GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mlops-titanic-app/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINER_DIR}/train.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import os\n",
    "import json\n",
    "\n",
    "def train_model(args):\n",
    "    # Input arguments\n",
    "    project_id = args.project_id\n",
    "    bucket_name = args.bucket_name\n",
    "    processed_data_path = args.processed_data_path\n",
    "    model_path = args.model_path\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "    n_hidden_layers = args.n_hidden_layers\n",
    "    n_units = args.n_units\n",
    "    activation_fn = args.activation_fn\n",
    "    test_data_file_name = args.test_data_file_name\n",
    "    \n",
    "    ### DATA PREPARATION SECTION ###\n",
    "    \n",
    "    # Get list of all Parquet files created by our preprocessing step in our GCS directory\n",
    "    fs = gcsfs.GCSFileSystem(project=project_id)  # replace with your project name\n",
    "    files = [f for f in fs.ls(processed_data_path) if 'part' in os.path.basename(f)]\n",
    "\n",
    "    print(f\"Found files: {files}\")\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No Parquet files found in directory: {processed_data_path}\")\n",
    "\n",
    "    # Read all Parquet files and concatenate into a single DataFrame\n",
    "    dfs = [pd.read_parquet('gs://' + file) for file in files]\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Separate the target and input features in the dataset\n",
    "    y = data['Survived'].values.astype('float32') # Ensuring the target column has a consistent data type\n",
    "    X = data.drop('Survived', axis=1)\n",
    "\n",
    "    # Convert X to NumPy array (required input for training)\n",
    "    X = X.values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    ### MODEL TRAINING AND EVALUATION SECTION ###\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_units, activation=activation_fn, input_shape=(X_train.shape[1],)))\n",
    "    for _ in range(n_hidden_layers - 1):\n",
    "        model.add(Dense(n_units, activation=activation_fn))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test)\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Get the model predictions\n",
    "    y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "    # Compute the AUC\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}, AUC: {auc}')\n",
    "\n",
    "    # Save the model to GCS\n",
    "    model.save(model_path)\n",
    "    \n",
    "    ### SAVING TEST DATA FOR LATER REFERENCE ###    \n",
    "    # Converting the test dataset to JSON Lines format and saving it to GCS\n",
    "    \n",
    "    # Convert numpy array to list of lists\n",
    "    X_test_list = X_test.tolist()\n",
    "    \n",
    "    # Create a JSONL string from the list of lists\n",
    "    jsonl_str = \"\\n\".join(json.dumps(instance) for instance in X_test_list)\n",
    "    \n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Get the bucket details\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    # Create the blob (See https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.blob.Blob)\n",
    "    blob = bucket.blob(test_data_file_name)\n",
    "    \n",
    "    # Upload the JSONL string to the blob\n",
    "    blob.upload_from_string(jsonl_str)\n",
    "    \n",
    "    # Return the trained model\n",
    "    return model  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train a neural network model for Titanic survival prediction')\n",
    "    \n",
    "    parser.add_argument('--project_id', type=str, help='GCP Project ID')\n",
    "    parser.add_argument('--bucket_name', type=str, help='GCP Bucket ID')\n",
    "    parser.add_argument('--processed_data_path', type=str, help='Path to the directory containing the preprocessed data')\n",
    "    parser.add_argument('--test_data_file_name', type=str, help='Path to the directory containing the preprocessed data')\n",
    "    parser.add_argument('--model_path', type=str, help='Path to save the trained model')\n",
    "    parser.add_argument('--n_hidden_layers', type=int, default=2, help='Number of hidden layers')\n",
    "    parser.add_argument('--n_units', type=int, default=64, help='Number of units per layer')\n",
    "    parser.add_argument('--activation_fn', type=str, default='relu', help='Activation function for hidden layers')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=20, help='Number of epochs')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our requirements.txt file\n",
    "The requirements.txt file is a convenient way to specify all of the packages that we want to install in our custom container image. This file will be referenced in the Dockerfile for our image.\n",
    "\n",
    "In this case, we will install:\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Python Client for Google Cloud Storage](https://cloud.google.com/python/docs/reference/storage/latest)\n",
    "* [Filesystem interfaces for Python](https://filesystem-spec.readthedocs.io/en/latest/)\n",
    "* [GCSFS](https://gcsfs.readthedocs.io/en/latest/)\n",
    "* [pyarrow](https://arrow.apache.org/docs/python/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mlops-titanic-app/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {APPLICATION_DIR}/requirements.txt\n",
    "google-cloud-aiplatform\n",
    "google-cloud-storage\n",
    "fsspec==2023.5.0\n",
    "gcsfs==2023.5.0\n",
    "pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dockerfile for our custom training container\n",
    "\n",
    "The [Dockerfile](https://docs.docker.com/engine/reference/builder/) specifies how to build our custom container image.\n",
    "\n",
    "This Dockerfile specifies that we want to:\n",
    "1. Use Vertex AI [prebuilt container for custom training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) as a base image.\n",
    "2. Install the required dependencied specified in our requirements.txt file.\n",
    "3. Copy our custom training script to the container image.\n",
    "4. Run our custom training script when the container starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mlops-titanic-app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {APPLICATION_DIR}/Dockerfile\n",
    "\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "COPY requirements.txt /requirements.txt\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copies the trainer code to the Docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our custom training image\n",
    "\n",
    "These steps are the same as the steps we performed above for our PySpark container, but in this case we are building our custom training container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Chapter-11-MLops/mlops-titanic-app\n"
     ]
    }
   ],
   "source": [
    "cd $APPLICATION_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:c685ebd7295abab98b38e7baf4a7b2dadaedb14d8c131e973378c297d8f1902b\n"
     ]
    }
   ],
   "source": [
    "! docker build ./ -t $TRAIN_IMAGE_URI --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push our custom image to Google Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/still-sight-352221/mlops-titanic-train/mlops-titanic-train]\n",
      "\n",
      "\u001b[1B7f0187c3: Preparing \n",
      "\u001b[1B12663cc0: Preparing \n",
      "\u001b[1B53c1883b: Preparing \n",
      "\u001b[1B76b5629f: Preparing \n",
      "\u001b[1B95c7b436: Preparing \n",
      "\u001b[1B967c8575: Preparing \n",
      "\u001b[1Bbcc2d347: Preparing \n",
      "\u001b[1B439bc91e: Preparing \n",
      "\u001b[1Bee080030: Preparing \n",
      "\u001b[1B84cdb93c: Preparing \n",
      "\u001b[1B9a008127: Preparing \n",
      "\u001b[1Bcc96b9f7: Preparing \n",
      "\u001b[1B37f6dd03: Preparing \n",
      "\u001b[1B3c0be4ce: Preparing \n",
      "\u001b[1B26e42ac3: Preparing \n",
      "\u001b[1B0a15895d: Preparing \n",
      "\u001b[1B98a818bd: Preparing \n",
      "\u001b[1B01f76958: Preparing \n",
      "\u001b[1B53965cbb: Preparing \n",
      "\u001b[1B66efbc81: Preparing \n",
      "\u001b[1B099a1ebc: Preparing \n",
      "\u001b[1B0c658f04: Preparing \n",
      "\u001b[1Bedee2eb8: Preparing \n",
      "\u001b[1Bdcc29187: Preparing \n",
      "\u001b[1Bcb1fd645: Preparing \n",
      "\u001b[2Bcb1fd645: Mounted from still-sight-352221/mlops-titanic-train-app/mlops-titanic-train \u001b[26A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:2f055b352087f27f4cc80dd55a22533f3ac1edb317aed6e773781355fb878622 size: 6378\n"
     ]
    }
   ],
   "source": [
    "! docker push $TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Chapter-11-MLops\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQd9M1_9bif7"
   },
   "source": [
    "# Define our Vertex AI Pipeline\n",
    "\n",
    "Now that we have defined our custom data preprocessing and model training components, it's time to define our MLOps pipeline.\n",
    "\n",
    "In this section, we will use the Kubeflow Pipelines SDK and Google Cloud Pipeline Components to define our MLOps pipeline.\n",
    "\n",
    "We begin by specifying all of the required variables in our pipeline, and populating their values from the constants we defined earlier in our notebook. We then specify the following components in our pipeline:\n",
    "\n",
    "1. [DataprocPySparkBatchOp](https://cloud.google.com/vertex-ai/docs/pipelines/dataproc-component) to perform our data preprocessing step.\n",
    "2. [CustomTrainingJobOp](https://cloud.google.com/vertex-ai/docs/pipelines/customjob-component#customjobop) to perform our custom model training step.\n",
    "3. [importer](https://www.kubeflow.org/docs/components/pipelines/v2/components/importer-component/) to import our [UnmanagedContainerModel](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.UnmanagedContainerModel) object.\n",
    "4. [ModelUploadOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/model.html#v1.model.ModelUploadOp) to upload our Model artifact into Vertex AI Model Registry.\n",
    "5. [EndpointCreateOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/endpoint.html#v1.endpoint.EndpointCreateOp) to create a Vertex AI [Endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints).\n",
    "6. [ModelDeployOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/endpoint.html#v1.endpoint.ModelDeployOp) to deploy our Google Cloud Vertex AI Model to an Endpoint, creating a [DeployedModel](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#deployedmodel) object within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yX2u9hXDWtpp"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=PIPELINE_NAME, description=\"MLOps pipeline for custom data preprocessing, model training, and deployment.\")\n",
    "def pipeline(\n",
    "    bucket_name: str = BUCKET,\n",
    "    display_name: str = PIPELINE_NAME,\n",
    "    preprocessing_main_python_file_uri: str = PREPROCESSING_PYTHON_FILE_URI,\n",
    "    preprocessing_args: list = PREPROCESSING_ARGS,\n",
    "    processed_data_path: str = PROCESSED_DATA_URI,\n",
    "    model_path: str = MODEL_URI,\n",
    "    custom_container_image: str = PYSPARK_IMAGE_URI,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    subnetwork_uri: str = SUBNETWORK_URI,\n",
    "    dataproc_runtime_version: str = DATAPROC_RUNTIME_VERSION,\n",
    "    worker_pool_specs: list = WORKER_POOL_SPEC,\n",
    "    base_output_directory: str = PIPELINE_ROOT,\n",
    "    serving_image_uri: str = SERVING_IMAGE_URI,\n",
    "    endpoint_name: str = ENDPOINT_NAME\n",
    "):\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessing_op = DataprocPySparkBatchOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        container_image=custom_container_image,\n",
    "        main_python_file_uri=preprocessing_main_python_file_uri,\n",
    "        args=preprocessing_args,\n",
    "        subnetwork_uri=subnetwork_uri,\n",
    "        runtime_config_version=dataproc_runtime_version,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model_training_op = custom_job.CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        display_name=\"train-mlops-model\",\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "    ).after(preprocessing_op)\n",
    "    \n",
    "    importer_op = dsl.importer(\n",
    "        artifact_uri=model_path,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": serving_image_uri,\n",
    "            },\n",
    "        },\n",
    "    ).after(model_training_op)\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=model_name,\n",
    "        unmanaged_container_model=importer_op.outputs[\"artifact\"],\n",
    "    ).after(importer_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=endpoint_name,\n",
    "    ).after(model_upload_op)\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_name,\n",
    "        dedicated_resources_machine_type=\"n1-standard-16\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    ).after(endpoint_create_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ePytY8t3bu"
   },
   "source": [
    "### Compile our pipeline into a YAML file\n",
    "\n",
    "Now that we have defined out pipeline structure, we need to compile it into YAML format in order to run it in Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uFyCaNPIWtsU"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline, 'mlops-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq26zYhQb0qm"
   },
   "source": [
    "## Submit and run our pipeline in Vertex AI Pipelines\n",
    "\n",
    "Now we're ready to use the Vertex AI Python SDK to submit and run our pipeline in Vertex AI Pipelines.\n",
    "\n",
    "The parameters, artifacts, and metrics produced from the pipeline run are automatically captured into Vertex AI Experiments as an experiment run. We will discuss the concept of Vertex AI Experiments in more detail in laer chapters in the book. The output of the following cell will provide a link at which you can watch your pipeline as it progresses through each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hwzYIoEabwRx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-titanic-pipeline-20230916202555?project=96449483013\n",
      "Associating projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555 to Experiment: aiml-sa-mlops-experiment\n"
     ]
    }
   ],
   "source": [
    "pipeline = aiplatform.PipelineJob(display_name=PIPELINE_NAME, template_path='mlops-pipeline.yaml')\n",
    "\n",
    "pipeline.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e428aab1826"
   },
   "source": [
    "### Wait for the pipeline to complete\n",
    "The following function will periodically print the status of our pipeline execution. If all goes to plan, you will eventually see a message saying \"PipelineJob run completed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b05ed7a9cf3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/96449483013/locations/us-central1/pipelineJobs/mlops-titanic-pipeline-20230916202555\n"
     ]
    }
   ],
   "source": [
    "pipeline.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!! You have officially created and implemented an MLOps pipeline on Vertex AI!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's send an inference request to our new model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been deployed to a Vertex AI Endpoint, we can start sending inference requests to it! In the real world, inference requests may come from a variety of potential sources. In our case, our training script created a small subset of our processed dataset to use for testing purposes. For convenience and example purposes, our training script also saved that test dataset to GCS. We can use it now to send inference requests to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the test dataset to a local directory in our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://kk-ml-book-us-actual-central-1/test_data.jsonl...\n",
      "/ [1 files][ 19.8 KiB/ 19.8 KiB]                                                \n",
      "Operation completed over 1 objects/19.8 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp $TEST_DATASET_PATH $TEST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model endpoint details\n",
    "\n",
    "In order to test our model, we first need to get the details of our newly-deployed model endpoint in Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/96449483013/locations/us-central1/endpoints/1927472470793650176\n"
     ]
    }
   ],
   "source": [
    "mlops_endpoint_list = aiplatform.Endpoint.list(filter=f'display_name={ENDPOINT_NAME}', order_by='create_time desc')\n",
    "new_mlops_endpoint = mlops_endpoint_list[0]\n",
    "endpoint_resource_name = new_mlops_endpoint.resource_name\n",
    "print(endpoint_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send inference requests to our model\n",
    "Let's go ahead and test it out! The following code will read a record from our test dataset and send it in an inference request to our model endpoint in Vertex AI.\n",
    "Our model should provide a prediction response that will be printed below the code cell. It should be a number between 0 and 1, which predicts the probability of survival for that record. Numbers closer to zero predict a low probability of survival, while numbers closer to 1 predict a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Prediction(predictions=[[0.233431324]], deployed_model_id='4937398745969983488', model_version_id='1', model_resource_name='projects/96449483013/locations/us-central1/models/6037257819420360704', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = LOCAL_TEST_DATASET_PATH\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    # Read the first line of the file\n",
    "    line = f.readline()\n",
    "\n",
    "    # Convert JSON line to Python dictionary\n",
    "    instance = json.loads(line)\n",
    "    \n",
    "    # Convert to a list of lists (required for our model input)\n",
    "    instance_list = [instance]\n",
    "\n",
    "    # Send the inference request\n",
    "    response = aiplatform.Endpoint(endpoint_resource_name).predict(instance_list)\n",
    "\n",
    "    # Print the response\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1169ce7bd4c8"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UovhFyLeelQe"
   },
   "outputs": [],
   "source": [
    "# Delete pipeline\n",
    "pipeline.delete()\n",
    "\n",
    "# Delete endpoints\n",
    "endpoint_list = aiplatform.Endpoint.list(filter=f'display_name=\"{ENDPOINT_NAME}\"')\n",
    "for endpoint in endpoint_list:\n",
    "    endpoint.undeploy_all()\n",
    "    endpoint.delete()\n",
    "\n",
    "# Delete model\n",
    "model_list = aiplatform.Model.list(filter=f'display_name=\"{MODEL_NAME}\"')\n",
    "for model in model_list:\n",
    "    model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4a76167aee4"
   },
   "outputs": [],
   "source": [
    "# Delete the Artifact repository\n",
    "! gcloud artifacts repositories delete $PYSPARK_REPO_NAME --location=$REGION --quiet\n",
    "! gcloud artifacts repositories delete $TRAIN_REPO_NAME --location=$REGION --quiet"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_cloud_pipeline_components_dataproc_tabular.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
